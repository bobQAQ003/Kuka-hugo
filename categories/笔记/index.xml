<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>笔记 on KukaDam</title>
        <link>https://bobqaq003.github.io/Kuka-hugo/categories/%E7%AC%94%E8%AE%B0/</link>
        <description>Recent content in 笔记 on KukaDam</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>KukaDam</copyright>
        <lastBuildDate>Wed, 29 Oct 2025 17:24:20 +0800</lastBuildDate><atom:link href="https://bobqaq003.github.io/Kuka-hugo/categories/%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Fairness] Grace：Graph self-distillation and completion to mitigate degree-related biases</title>
        <link>https://bobqaq003.github.io/Kuka-hugo/p/fairness-gracegraph-self-distillation-and-completion-to-mitigate-degree-related-biases/</link>
        <pubDate>Wed, 29 Oct 2025 16:32:18 +0800</pubDate>
        
        <guid>https://bobqaq003.github.io/Kuka-hugo/p/fairness-gracegraph-self-distillation-and-completion-to-mitigate-degree-related-biases/</guid>
        <description>&lt;blockquote class=&#34;alert alert-note&#34;&gt;
    &lt;p&gt;论文来自：&lt;/p&gt;
&lt;p&gt;Xu H, Xiang L, Huang F, et al. Grace: Graph self-distillation and completion to mitigate degree-related biases[C]//Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023: 2813-2824.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要
&lt;/h2&gt;&lt;p&gt;真实世界图普遍呈现&lt;strong&gt;长尾度分布&lt;/strong&gt;（long-tail degree distribution），大量节点为&lt;strong&gt;低度节点&lt;/strong&gt;（low-degree nodes）。尽管 GNN 在节点分类任务上表现出色，但其性能严重依赖丰富连接，&lt;strong&gt;对低度节点表示不足&lt;/strong&gt;，导致显著的&lt;strong&gt;度相关偏差（degree-related bias）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;本文提出 &lt;strong&gt;Grace&lt;/strong&gt;，通过以下两大机制缓解该问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;自蒸馏（Graph Self-Distillation）&lt;/strong&gt;：增强低度节点的&lt;strong&gt;自表示能力（self-representation）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图补全（Graph Completion）&lt;/strong&gt;：提升低度节点的&lt;strong&gt;邻域同质性比率（Neighborhood Homophily Ratio, NHR）&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;结合 &lt;strong&gt;标签传播（Label Propagation）&lt;/strong&gt; 防止错误传播。实验表明，Grace 在平衡整体性能与低度节点准确率方面显著优于现有方法。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;问题背景与动机&#34;&gt;问题背景与动机
&lt;/h2&gt;&lt;h3 id=&#34;图1真实图的度分布与性能偏差&#34;&gt;图1：真实图的度分布与性能偏差
&lt;/h3&gt;&lt;p&gt;



&lt;div class=&#34;post-img-view&#34;&gt;
	&lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://s2.loli.net/2025/10/29/w6Gzyj312XMhgCK.png&#34;&gt;
		&lt;img src=&#34;https://s2.loli.net/2025/10/29/w6Gzyj312XMhgCK.png&#34; alt=&#34;image-20251029164055547&#34;  /&gt;
	&lt;/a&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;核心挑战&#34;&gt;核心挑战
&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;挑战&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;① 自表示不足&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;GNN 过度依赖邻域聚合，低度节点失去邻域后性能崩塌至 MLP 水平&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;② 低 NHR&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;低度节点邻居中同类节点比例极低，违反 GNN 的同质性假设&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;整体框架&#34;&gt;整体框架
&lt;/h2&gt;&lt;p&gt;



&lt;div class=&#34;post-img-view&#34;&gt;
	&lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://s2.loli.net/2025/10/29/9KjLt1TpcWm3Ada.png&#34;&gt;
		&lt;img src=&#34;https://s2.loli.net/2025/10/29/9KjLt1TpcWm3Ada.png&#34; alt=&#34;image-20251029164240677&#34;  /&gt;
	&lt;/a&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;流程&#34;&gt;流程
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;分解 GNN&lt;/strong&gt; 将 GNN 分成 &lt;strong&gt;ST（自变换，MLP）&lt;/strong&gt; 和 &lt;strong&gt;NT（邻域变换）&lt;/strong&gt; 两部分。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自蒸馏训练&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;教师模型&lt;/strong&gt;：完整 GNN（ST + NT）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学生模型&lt;/strong&gt;：仅 ST（退化为 MLP）&lt;/li&gt;
&lt;li&gt;用教师软标签 + 真实标签监督学生，优化 ST 学习“邻域平移”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图补全（Graph Completion）&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;对低度节点 $ v $，用自蒸馏输出 $ p(v) $ 预测同类邻居&lt;/li&gt;
&lt;li&gt;建模为多标签任务：正样本=当前邻居，负样本=低相似度节点&lt;/li&gt;
&lt;li&gt;选前 $ k $ 个高概率同类节点，添加新边，构建新图 $ G&amp;rsquo; $。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练 Grace-GNN&lt;/strong&gt; 在补全后的图 $ G&amp;rsquo; $ 上，用增强的 ST + NT 训练最终 GNN。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理阶段：标签传播&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;对低度节点，用 $ p(v) $ 选前 $ k $ 个邻居&lt;/li&gt;
&lt;li&gt;构建&lt;strong&gt;有向边&lt;/strong&gt;（高置信 → 低度）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;单跳传播&lt;/strong&gt;： $\hat{p}(v) = (1-\lambda) p(v) + \lambda \sum_{u \to v} p(u)$&lt;/li&gt;
&lt;li&gt;输出最终预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;核心特点&lt;/strong&gt;：&lt;strong&gt;自蒸馏增强自表示&lt;/strong&gt; + &lt;strong&gt;精准补全提升 NHR&lt;/strong&gt; + &lt;strong&gt;单跳传播防误传&lt;/strong&gt;，实现低度节点性能大幅提升，整体无损。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;核心模块&#34;&gt;核心模块
&lt;/h2&gt;&lt;h3 id=&#34;自蒸馏graph-self-distillation&#34;&gt;自蒸馏（Graph Self-Distillation）
&lt;/h3&gt;&lt;h4 id=&#34;思路&#34;&gt;思路
&lt;/h4&gt;&lt;p&gt;将 GNN 分解为两部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ST（Self-Transformation）&lt;/strong&gt;：节点自身特征变换（MLP）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NT（Neighborhood Transformation）&lt;/strong&gt;：邻居信息聚合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;目标：将图依赖性&lt;strong&gt;从 NT 迁移到 ST&lt;/strong&gt;，使 ST 学习到“邻域平移”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;实现&#34;&gt;实现
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;教师模型&lt;/strong&gt;：完整 GNN（含 ST + NT）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学生模型&lt;/strong&gt;：仅 ST 部分（退化为 MLP）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练流程&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;输入节点特征 → 教师模型 → 软标签 $ p_t(v) $&lt;/li&gt;
&lt;li&gt;输入相同特征 → 学生模型 → 输出 $ p_s(v) $&lt;/li&gt;
&lt;li&gt;损失： $\mathcal{L}_{KD} = \sum_v \left[ \alpha \cdot KL(p_t(v) | p_s(v)) + (1-\alpha) \cdot CE(p_s(v), y_v) \right]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;收敛后，&lt;strong&gt;ST 隐式编码了邻域信息&lt;/strong&gt;，增强低度节点自表示&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;图补全graph-completion&#34;&gt;图补全（Graph Completion）
&lt;/h3&gt;&lt;h4 id=&#34;目标&#34;&gt;目标
&lt;/h4&gt;&lt;p&gt;为低度节点预测&lt;strong&gt;同类潜在邻居&lt;/strong&gt;，提升 NHR&lt;/p&gt;
&lt;h4 id=&#34;建模为多标签预测任务&#34;&gt;建模为多标签预测任务
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;正样本&lt;/strong&gt;：当前邻居 $ \mathcal{N}(v) $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;候选负样本&lt;/strong&gt;：其他节点&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测&lt;/strong&gt;：使用自蒸馏输出的软标签 $ p(v) $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选择策略&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;取 $ p(v) $ 中前 2 大概率类别&lt;/li&gt;
&lt;li&gt;归一化得到权重&lt;/li&gt;
&lt;li&gt;负样本：与 $ v $ 余弦相似度 &amp;lt; $ \eta $&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最终标签&lt;/strong&gt;： $y_v^{\text{multi}} = \text{softmax}\left( \sum_{u \in \mathcal{N}(v)} p(u) \right)$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;仅连接&lt;strong&gt;最可能同类的节点&lt;/strong&gt;，避免噪声&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;标签传播label-propagation&#34;&gt;标签传播（Label Propagation）
&lt;/h3&gt;&lt;h4 id=&#34;作用&#34;&gt;作用
&lt;/h4&gt;&lt;p&gt;防止图补全中的误分类传播&lt;/p&gt;
&lt;h4 id=&#34;预测阶段流程&#34;&gt;预测阶段流程
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;根据 $ p(v) $ 选前 $ k $ 个潜在邻居&lt;/li&gt;
&lt;li&gt;构建有向边：$ u \to v $（$ u $ 是高置信邻居）&lt;/li&gt;
&lt;li&gt;单跳传播： $\hat{p}(v) = (1 - \lambda) p(v) + \lambda \sum_{u \in \mathcal{N}&amp;rsquo;(v)} p(u)$ 其中 $ \mathcal{N}&amp;rsquo;(v) $ 为新图中的邻居&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;信息&lt;strong&gt;单向流动&lt;/strong&gt;：从可靠节点 → 低度节点&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;符号表&#34;&gt;符号表
&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;符号&lt;/th&gt;
          &lt;th&gt;含义&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;$ G = (V, E, X, Y) $&lt;/td&gt;
          &lt;td&gt;原始图&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \deg(v) $&lt;/td&gt;
          &lt;td&gt;节点 $ v $ 的度数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \text{NHR}(v) $&lt;/td&gt;
          &lt;td&gt;邻域同质性比率 = 同类邻居数 / 总邻居数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \text{ST}(\cdot), \text{NT}(\cdot) $&lt;/td&gt;
          &lt;td&gt;自变换、邻域变换&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ p_t(v), p_s(v) $&lt;/td&gt;
          &lt;td&gt;教师/学生模型输出概率&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \mathcal{L}_{KD} $&lt;/td&gt;
          &lt;td&gt;知识蒸馏损失&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \eta $&lt;/td&gt;
          &lt;td&gt;负样本相似度阈值&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ k $&lt;/td&gt;
          &lt;td&gt;预测邻居数量&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ G&amp;rsquo; $&lt;/td&gt;
          &lt;td&gt;补全后新图&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \mathcal{N}&amp;rsquo;(v) $&lt;/td&gt;
          &lt;td&gt;新图中 $ v $ 的邻居集&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>[Privacy] LPGNN：Locally private graph neural networks</title>
        <link>https://bobqaq003.github.io/Kuka-hugo/p/privacy-lpgnnlocally-private-graph-neural-networks/</link>
        <pubDate>Fri, 24 Oct 2025 15:58:38 +0800</pubDate>
        
        <guid>https://bobqaq003.github.io/Kuka-hugo/p/privacy-lpgnnlocally-private-graph-neural-networks/</guid>
        <description>&lt;blockquote class=&#34;alert alert-note&#34;&gt;
    &lt;p&gt;论文来自于：&lt;/p&gt;
&lt;p&gt;Sajadmanesh S, Gatica-Perez D. Locally private graph neural networks[C]//Proceedings of the 2021 ACM SIGSAC conference on computer and communications security. 2021: 2130-2145.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要
&lt;/h2&gt;&lt;p&gt;图神经网络（GNN）在学习节点表示以完成多种图推理任务时表现出色。然而，当节点代表个人或涉及敏感信息的人类相关变量时，基于图数据的学习可能引发隐私问题。尽管已有大量针对非关系数据的隐私保护深度学习技术，但针对图上深度学习算法的隐私问题研究较少。&lt;/p&gt;
&lt;p&gt;本文研究&lt;strong&gt;节点数据隐私问题&lt;/strong&gt;：图节点拥有潜在敏感数据（特征向量与标签）保持私有，但可被中心服务器用于训练GNN。提出一种基于&lt;strong&gt;局部差分隐私&lt;/strong&gt;（LDP）的隐私保护GNN训练算法，具备形式化的隐私保证。&lt;/p&gt;
&lt;h3 id=&#34;核心贡献&#34;&gt;核心贡献
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;多比特机制（Multi-bit Mechanism）&lt;/strong&gt;：LDP编码器 + 无偏整流器，实现高效通信下特征扰动与收集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KProp层&lt;/strong&gt;：基于多跳聚合的简单图卷积层，增强去噪能力，提升首层卷积估计精度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Drop鲁棒训练框架&lt;/strong&gt;：利用KProp对噪声标签进行去噪，提升泛化性能。&lt;/li&gt;
&lt;li&gt;理论分析：隐私保证、误差界。&lt;/li&gt;
&lt;li&gt;实验验证：在真实数据集上实现良好的&lt;strong&gt;准确率-隐私权衡&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;问题定义与背景&#34;&gt;问题定义与背景
&lt;/h2&gt;&lt;h3 id=&#34;图结构&#34;&gt;图结构
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;图 $ G = (V, E, X, Y) $
&lt;ul&gt;
&lt;li&gt;$ V = V_L \cup V_U $：有标签节点 + 无标签节点&lt;/li&gt;
&lt;li&gt;$ X \in \mathbb{R}^{|V| \times d} $：特征矩阵（私有）&lt;/li&gt;
&lt;li&gt;$ Y \in {0,1}^{|V| \times c} $：标签矩阵（私有）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;服务器拥有：$ V, E $&lt;/li&gt;
&lt;li&gt;节点私有：$ X, Y $&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;目标&#34;&gt;目标
&lt;/h3&gt;&lt;p&gt;在不泄露原始 $ X, Y $ 的前提下，训练GNN。&lt;/p&gt;
&lt;h2 id=&#34;图神经网络gnn基础&#34;&gt;图神经网络（GNN）基础
&lt;/h2&gt;&lt;p&gt;每层嵌入更新：&lt;/p&gt;
$$
h^l_{N(v)} = \text{Aggregate}^l \left( \{ h^{l-1}_u \mid \forall u \in \mathcal{N}(v) \} \right)
$$$$
h^l_v = \text{Update}^l \left( h^l_{N(v)} \right)
$$&lt;ul&gt;
&lt;li&gt;初始：$ h^0_v = x_v $&lt;/li&gt;
&lt;li&gt;最后一层输出 $ c $ 维向量 + softmax 预测标签&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;局部差分隐私ldp&#34;&gt;局部差分隐私（LDP）
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;定义 2.1&lt;/strong&gt;：机制 $ M $ 满足 $ \epsilon $-LDP，若对任意输入 $ x, x&amp;rsquo; $ 和输出 $ y $：&lt;/p&gt;
$$
\Pr[M(x)=y] \leq e^\epsilon \Pr[M(x&#39;)=y]
$$&lt;ul&gt;
&lt;li&gt;$ \epsilon $：隐私预算（越小越强）&lt;/li&gt;
&lt;li&gt;实现方式：本地扰动 → 聚合去噪&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;整体框架&#34;&gt;整体框架
&lt;/h2&gt;&lt;p&gt;



&lt;div class=&#34;post-img-view&#34;&gt;
	&lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://s2.loli.net/2025/10/29/g9lNDuoBvGJ5zxQ.png&#34;&gt;
		&lt;img src=&#34;https://s2.loli.net/2025/10/29/g9lNDuoBvGJ5zxQ.png&#34; alt=&#34;image-20251029165231143&#34;  /&gt;
	&lt;/a&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;流程&#34;&gt;流程
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;节点本地扰动&lt;/strong&gt; 每个节点用 &lt;strong&gt;Multi-bit Encoder&lt;/strong&gt; 对特征进行局部扰动，生成编码向量；有标签节点用 &lt;strong&gt;广义随机响应&lt;/strong&gt; 扰动标签。一次性上传至服务器。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;服务器去偏估计&lt;/strong&gt; 服务器用 &lt;strong&gt;Multi-bit Rectifier&lt;/strong&gt; 对所有扰动向量进行无偏校正，估计近似首层卷积输入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;插入 KProp 去噪&lt;/strong&gt; 在 GNN 前插入 &lt;strong&gt;KProp 多跳聚合层&lt;/strong&gt;，通过 K 步线性传播平均化噪声，增强低度节点表示。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GNN 训练（Drop 鲁棒框架）&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;前向传播得到预测概率&lt;/li&gt;
&lt;li&gt;用 KProp 聚合预测概率，生成“净化标签”&lt;/li&gt;
&lt;li&gt;结合扰动标签 + 净化标签计算损失&lt;/li&gt;
&lt;li&gt;反向传播更新模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代至收敛&lt;/strong&gt; 每轮重复扰动→上传→去偏→训练，直至模型收敛。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推理&lt;/strong&gt; 训练好的 GNN 直接用于新节点分类（无需再扰动）。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;核心特点&lt;/strong&gt;：&lt;strong&gt;一次通信&lt;/strong&gt; + &lt;strong&gt;聚合去噪&lt;/strong&gt; + &lt;strong&gt;标签净化&lt;/strong&gt;，实现 &lt;strong&gt;隐私保护下的高精度 GNN 训练&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;核心组件&#34;&gt;核心组件
&lt;/h2&gt;&lt;h3 id=&#34;multi-bit-encoder节点端&#34;&gt;Multi-bit Encoder（节点端）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：节点 $ v $ 的原始特征 $ x_v \in \mathbb{R}^d $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：扰动编码向量&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：高维特征下高效通信 + LDP保证&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：扩展1-bit机制至多维，逐维度采样并翻转&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;每个节点仅需&lt;strong&gt;一次通信&lt;/strong&gt;发送扰动特征&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;multi-bit-rectifier服务器端&#34;&gt;Multi-bit Rectifier（服务器端）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：所有节点的扰动编码&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：无偏估计的聚合特征&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：校正统计偏差，近似首层图卷积&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键&lt;/strong&gt;：利用&lt;strong&gt;线性聚合&lt;/strong&gt;作为天然去噪机制&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;聚合平均化注入的差分隐私噪声&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;kprop多跳聚合层&#34;&gt;KProp（多跳聚合层）
&lt;/h3&gt;$$
h_v^{(k)} = (1 - \alpha) h_v^{(k-1)} + \alpha \cdot \text{Aggregate}(h_u^{(k-1)}, u \in \mathcal{N}(v))
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：扰动后特征&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;操作&lt;/strong&gt;：迭代K次线性聚合，扩展有效邻域至K跳&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;增强去噪（噪声 ~ 1/√聚合规模）&lt;/li&gt;
&lt;li&gt;提升低度节点估计精度&lt;/li&gt;
&lt;li&gt;可插入任意GNN前作为预处理层&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;特别适用于&lt;strong&gt;幂律分布图&lt;/strong&gt;（多数节点度数低）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;标签扰动randomized-response&#34;&gt;标签扰动（Randomized Response）
&lt;/h3&gt;&lt;p&gt;正确标签保留概率：$ p = \frac{e^\epsilon}{e^\epsilon + c - 1} $&lt;/p&gt;
&lt;p&gt;其他类别随机翻转为：$ q = \frac{1}{e^\epsilon + c - 1} $&lt;/p&gt;
&lt;h3 id=&#34;drop鲁棒训练框架&#34;&gt;Drop：鲁棒训练框架
&lt;/h3&gt;&lt;h4 id=&#34;挑战&#34;&gt;挑战
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;噪声标签 → 过拟合 → 泛化差&lt;/li&gt;
&lt;li&gt;无干净验证集 → 难以调参&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;解决方案&#34;&gt;解决方案
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;利用KProp对&lt;strong&gt;标签预测概率&lt;/strong&gt;进行多跳聚合&lt;/li&gt;
&lt;li&gt;估计每个节点的&lt;strong&gt;邻域标签频率&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;选择频率最高者作为“净化标签”&lt;/li&gt;
&lt;li&gt;用于损失计算或早停判断&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;不依赖任何干净数据（特征/标签）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;符号表文中主要符号&#34;&gt;符号表（文中主要符号）
&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;符号&lt;/th&gt;
          &lt;th&gt;含义&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;$ G = (V, E, X, Y) $&lt;/td&gt;
          &lt;td&gt;图结构&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ V_L, V_U $&lt;/td&gt;
          &lt;td&gt;有/无标签节点集&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ x_v \in \mathbb{R}^d $&lt;/td&gt;
          &lt;td&gt;节点 $ v $ 的特征向量&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ y_v \in {0,1}^c $&lt;/td&gt;
          &lt;td&gt;节点 $ v $ 的one-hot标签&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \mathcal{N}(v) $&lt;/td&gt;
          &lt;td&gt;节点 $ v $ 的邻居集（可含自身）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ h^l_v $&lt;/td&gt;
          &lt;td&gt;第 $ l $ 层节点 $ v $ 的嵌入&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \epsilon $&lt;/td&gt;
          &lt;td&gt;隐私预算&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ K $&lt;/td&gt;
          &lt;td&gt;KProp 步数&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ M(\cdot) $&lt;/td&gt;
          &lt;td&gt;LDP扰动机制&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \text{MB-Encoder} $&lt;/td&gt;
          &lt;td&gt;多比特编码器&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$ \text{MB-Rectifier} $&lt;/td&gt;
          &lt;td&gt;多比特整流器&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>[Privacy] 自适应隐私预算分配</title>
        <link>https://bobqaq003.github.io/Kuka-hugo/p/privacy-%E8%87%AA%E9%80%82%E5%BA%94%E9%9A%90%E7%A7%81%E9%A2%84%E7%AE%97%E5%88%86%E9%85%8D/</link>
        <pubDate>Thu, 23 Oct 2025 19:00:59 +0800</pubDate>
        
        <guid>https://bobqaq003.github.io/Kuka-hugo/p/privacy-%E8%87%AA%E9%80%82%E5%BA%94%E9%9A%90%E7%A7%81%E9%A2%84%E7%AE%97%E5%88%86%E9%85%8D/</guid>
        <description>&lt;h2 id=&#34;按度数分布划分区间总隐私预算以区间&#34;&gt;按度数分布划分区间，总隐私预算以区间
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Yuan Y, Lei D, Fan Q, et al. Achieving Adaptive Privacy-Preserving Graph Neural Networks Training in Cloud Environment[C]//2024 IEEE 12th International Conference on Information, Communication and Networks (ICICN). IEEE, 2024: 181-186.&lt;/p&gt;
&lt;p&gt;Yuan Y, Lei D, Zhang C, et al. Personalized differential privacy graph neural network[J]. IEEE/CAA Journal of Automatica Sinica, 2025.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;给出一个总体区间 $[ε_1, ε_2]$，再按&lt;strong&gt;节点度数&lt;/strong&gt;把用户分桶，并把 $[ε_1, ε_2]$切成多个&lt;strong&gt;子区间&lt;/strong&gt;；每个用户的“初始”隐私预算 $ε_s$ 会&lt;strong&gt;从对应子区间里随机采样&lt;/strong&gt;（按指数分布采样），因此不同用户拿到的起始预算彼此不同&lt;/p&gt;
&lt;p&gt;根据度数的大小，我们可以将这些节点划分成几个区间。例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;区间1：度数小于等于 10&lt;/li&gt;
&lt;li&gt;区间2：度数大于 10 且小于等于 50&lt;/li&gt;
&lt;li&gt;区间3：度数大于 50&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据区间的划分，隐私预算会在区间 $[ε₁, ε₂]$ 内进行分配。假设$ε₁ = 0.1$，$ε₂ = 1$，并且使用&lt;strong&gt;指数分布&lt;/strong&gt;来决定隐私预算的具体值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于区间1，为其分配较高的隐私预算（如接近$ε₂ = 1$）。&lt;/li&gt;
&lt;li&gt;对于区间，隐私预算会适中。&lt;/li&gt;
&lt;li&gt;对于区间3，隐私预算会较低（接近$ε₁ = 0.1$）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;隐私预算的分配在度数区间的划分上使用了&lt;strong&gt;指数分布&lt;/strong&gt;。其公式为：
&lt;/p&gt;
$$
\text{Sample from exponential distribution } f(y, \lambda) = \lambda e^{-\lambda y}, \, (y \geq 0)
$$&lt;p&gt;
之后用户用各自的 $ε_s$ 加噪：$\hat X_s=f(X_s)+\mathrm{Lap}(\Delta f/ε_s)$，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;APPGNN&lt;/strong&gt;按照&lt;strong&gt;节点度数&lt;/strong&gt;划分为若干个区间，并将 &lt;strong&gt;隐私预算区间&lt;/strong&gt; 对应划分，根据节点度数的 &lt;strong&gt;比例分布&lt;/strong&gt;，将其 &lt;strong&gt;映射到指数分布的分位数区间&lt;/strong&gt;，从中 &lt;strong&gt;采样出个性化的隐私预算&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;拓扑重要性个性化总隐私预算以区间&#34;&gt;拓扑重要性个性化，总隐私预算以区间
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Lei D, Song Z, Yuan Y, et al. Achieving Personalized Privacy-Preserving Graph Neural Network via Topology Awareness[C]//Proceedings of the ACM on Web Conference 2025. 2025: 3552-3560.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;提出“邻接信息熵”（Adjacency Information Entropy, AIE）来衡量节点拓扑重要性，既考虑直连邻居也考虑间接关系：先算邻接度 $AD_u=\sum_{v\in \mathcal N_u} D_v$，再定义概率 $p_u=D_u/AD_v$，最后得信息熵式的 $AIE_u=-\sum_{v\in \mathcal N_u}(p_u\log_2 p_u),p_v$（式(4)–(6)）。重要性越高→隐私敏感度越高。&lt;/p&gt;
&lt;p&gt;将总预算区间 $[,\epsilon_b,\epsilon_e,]$ 按节点隐私敏感度分成 $M$ 个等级与对应&lt;strong&gt;子区间&lt;/strong&gt;，并假设预算在该区间内&lt;strong&gt;服从指数分布&lt;/strong&gt;（真实网络度分布常呈幂律，少数节点很重要）。通过指数分布分位点把 $[,\epsilon_b,\epsilon_e,]$ 切成 $(\epsilon_b,\epsilon_1],(\epsilon_1,\epsilon_2],\dots,(\epsilon_{M-2},\epsilon_e]$，切分边界用式(7)(8)的分位数关系确定；重要节点→分到&lt;strong&gt;更小的 $\epsilon$&lt;/strong&gt;（更强保护/更大噪声），不太重要的节点→更大的 $\epsilon$（更少噪声）。每个节点最终&lt;strong&gt;从其子区间随机采样&lt;/strong&gt;得到个性化预算 $\epsilon_i$。&lt;/p&gt;
&lt;p&gt;各节点用自己的 $\epsilon_i$ 做拉普拉斯机制：$\hat X_i=f(X_i)+\mathrm{Lap}(\Delta f/\epsilon_i)$（式(3)、(9)），并用&lt;strong&gt;随机响应&lt;/strong&gt;扰动标签（式(12)），实现特征+标签双重保护。&lt;/p&gt;
&lt;p&gt;因不同邻居被加的噪声强度不同，直接平均会放大高噪声邻居的负面影响。论文据“&lt;strong&gt;越重要→预算越小→噪声越大&lt;/strong&gt;”的链条，给重要邻居更小权重：
&lt;/p&gt;
$$
W_{u,v}=1+\frac{1}{D_u}-\frac{AIE_v}{\sum_{i\in Ner_u}AIE_{u,i}+AIE_u},
$$&lt;p&gt;
再做加权聚合（式(10)(11)）。这样能&lt;strong&gt;抑制差异化DP噪声&lt;/strong&gt;对表示学习的影响。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TDP-GNN 通过拓扑结构识别节点重要性，划分为多个隐私敏感度等级，映射到指数分布的预算区间并采样个性化预算，再结合加权聚合抑制噪声。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;上下文多臂赌博机cmab算法分配区间&#34;&gt;上下文多臂赌博机（CMAB）算法分配（区间）
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Zhang X, Zhou Y, Hu M, et al. BGTplanner: Maximizing Training Accuracy for Differentially Private Federated Recommenders via Strategic Privacy Budget Allocation[J]. IEEE Transactions on Services Computing, 2025.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;步骤 1：生成奖励的预测&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于每个动作 $a_t$ 和上下文 $X_t$，BGTplanner使用高斯过程回归模型预测奖励 $r_t$：
$$
    \mu(z_t) = \text{GPR}(a_t, X_t)
    $$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;步骤 2：计算动作的评分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据预测的奖励 $\mu(z_t)$，为每个可能的动作（隐私预算分配方案）计算一个&lt;strong&gt;评分&lt;/strong&gt; $\beta_t(a)$，公式如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\beta_t(a) = \mu(z_t) - \langle \epsilon_{\text{total}} - \epsilon_t, \lambda_t \rangle
$$&lt;p&gt;​	其中，$\lambda_t$ 是用于&lt;strong&gt;长期隐私预算约束&lt;/strong&gt;的拉格朗日乘子，确保在整个训练过程中不会超出总预算。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;步骤 3：选择最优的隐私预算分配方案
&lt;ul&gt;
&lt;li&gt;$\gamma$ 是&lt;strong&gt;探索和利用的权衡参数&lt;/strong&gt;，控制着系统是否偏向于选择当前最优的动作（利用）或探索其他可能的动作。&lt;/li&gt;
&lt;li&gt;$A$ 是&lt;strong&gt;动作空间&lt;/strong&gt;的大小，表示可能的隐私预算分配方案的数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;步骤 4：更新隐私预算消耗
&lt;ul&gt;
&lt;li&gt;在每轮训练之后，BGTplanner通过&lt;strong&gt;隐私预算消耗函数&lt;/strong&gt;来计算实际消耗的隐私预算 $\epsilon_t$，并更新剩余预算。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;假设我们有如下参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;总隐私预算&lt;/strong&gt; $\epsilon_{\text{total}} = 10$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私预算区间&lt;/strong&gt;：$\epsilon_{\text{min}} = 1, \epsilon_{\text{max}} = 5$&lt;/li&gt;
&lt;li&gt;每轮的隐私预算分配动作是从区间 $[1, 5]$ 中选择的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在某个训练回合中，BGTplanner预测奖励为：&lt;/p&gt;
&lt;/blockquote&gt;
$$
\mu(z_t) = 0.8 \quad (\text{基于历史信息和上下文的奖励预测})
$$&lt;blockquote&gt;
&lt;p&gt;接着，BGTplanner计算所有可能动作的评分，并选择评分最高的动作。例如，假设动作 $a_1$ 得分为 0.9，动作 $a_2$ 得分为 0.7，最终选择 $a_1$ 作为隐私预算分配方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BGTplanner 每轮都用 CMAB 从一组预算选项中，智能选一个最合适的隐私预算来用&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;探讨隐私预算的推荐值&#34;&gt;探讨隐私预算的推荐值
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Du W, Ma X, Dong W, et al. Calibrating privacy budgets for locally private graph neural networks[C]//2021 International Conference on Networking and Network Applications (NaNA). IEEE, 2021: 23-29.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;采用了 &lt;strong&gt;Multi-bit LDP Mechanism&lt;/strong&gt; (LPGNN的方法)对用户特征进行扰动:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用户特征向量$ x∈[α,β]^d$&lt;/li&gt;
&lt;li&gt;隐私预算$\epsilon$&lt;/li&gt;
&lt;li&gt;控制参数 &lt;em&gt;m&lt;/em&gt;（每次扰动的特征维度数）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;输出：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;扰动后的特征向量 $ x∈{-1,0,1}^d$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过&lt;strong&gt;链路预测准确率&lt;/strong&gt;和加入属性推断攻击后的&lt;strong&gt;F1-score值&lt;/strong&gt;推荐隐私预算值&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;项目&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;内容&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;目标&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;在 LDP 保护的 GNN 中合理选择隐私预算 ε&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;方法&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;利用属性推断攻击效果作为隐私度量，结合链路预测准确率评估效用&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;隐私机制&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Multi-bit LDP 机制，用户本地扰动特征，服务器无偏重构&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;推荐 ε 值&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;0.5 ~ 1（视具体业务对隐私和效用的需求）&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;基于遗传算法ga的隐私预算分配&#34;&gt;基于遗传算法（GA）的隐私预算分配
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Li Y, Song X, Tu Y, et al. GAPBAS: Genetic algorithm-based privacy budget allocation strategy in differential privacy K-means clustering algorithm[J]. Computers &amp;amp; Security, 2024, 139: 103697.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过分析噪声对质心的影响，推导出 &lt;strong&gt;最小隐私预算$ε_m$&lt;/strong&gt;：
&lt;/p&gt;
$$
ε_m=(\frac{200k^3d+(1+d)^2}{N^2}(1+ρ^2))^{1/2}
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;k&lt;/em&gt;：聚类数；&lt;em&gt;d&lt;/em&gt;：数据维度；&lt;em&gt;N&lt;/em&gt;：样本数量；&lt;em&gt;ρ&lt;/em&gt;：噪声相关系数（通常取 0.225）；&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;每轮预算必须满足：&lt;em&gt;$ε_t$&lt;/em&gt;≥&lt;em&gt;$ε_m$&lt;/em&gt;，否则噪声过大导致质心不收敛。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;每个个体是一个长度为 $T$ 的浮点数组：${ε_1,ε_2,&amp;hellip;,ε_T}$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;GAPBAS 将每轮隐私预算组合成序列，作为遗传算法的个体，在满足总预算和最小预算约束下，优化出使聚类效果（NICV）最优的预算分配策略。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;基于隐私安全等级privacy-security-level-psl&#34;&gt;基于隐私安全等级（Privacy Security Level, PSL）
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Shen Z, He S, Wang H, et al. A differential privacy budget allocation method combining privacy security level[J]. Journal of Communications and Information Networks, 2023, 8(1): 90-98.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;提出 &lt;strong&gt;PSL 方法&lt;/strong&gt;：为每个位置分配一个“隐私安全等级”，并据此动态分配隐私预算 $ε$，实现 &lt;strong&gt;个性化、拓扑感知的隐私保护&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用 &lt;strong&gt;P-series（p-级数）&lt;/strong&gt; 为初始敏感位置分配隐私预算&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用 &lt;strong&gt;P-series&lt;/strong&gt; 为初始敏感位置分配预算：
&lt;/p&gt;
$$
     \varepsilon_{m} = \frac{\varepsilon}{\zeta(p)} \times \frac{1}{m^{p}}, \quad m \in \mathbb{N}^{+}, \quad p &gt; 1
     $$&lt;ul&gt;
&lt;li&gt;$\varepsilon$：总隐私预算；&lt;/li&gt;
&lt;li&gt;$\zeta(p)$：P级数收敛值（如 p=2 时，$\zeta(2)= π²/6$；&lt;/li&gt;
&lt;li&gt;&lt;em&gt;m&lt;/em&gt;：敏感位置编号（按重要性排序）；&lt;/li&gt;
&lt;li&gt;结果：重要位置（m 小）获得更大预算（更小噪声）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据 &lt;strong&gt;距离与节点度&lt;/strong&gt; 为敏感点的邻居分配预算，并支持 &lt;strong&gt;动态时间调整&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PSL定义：
&lt;/p&gt;
$$
     \mathrm{PSL}(k_{m}) = \lambda \times \varepsilon_{m} = \lambda \times \frac{\varepsilon}{\zeta(p)} \times m^{p}
     $$&lt;ul&gt;
&lt;li&gt;PSL 与预算$\varepsilon$ 成反比；&lt;/li&gt;
&lt;li&gt;$\lambda$ 为调节参数;&lt;/li&gt;
&lt;li&gt;用于衡量位置的“隐私敏感度”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将 PSL 映射为隐私预算$ε$，满足 $\epsilon × PSL = \gamma$（$\gamma$ 为常数）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;敏感节点 $k_1$ 的 PSL = 0.6079；&lt;/li&gt;
&lt;li&gt;邻居 A 距离为 1，邻居 B 距离为 2；&lt;/li&gt;
&lt;li&gt;NPS = {A, B}；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;则：
&lt;/p&gt;
$$
\mathrm{PSL}_{A} = \frac{1/1}{(1/1 + 1/2)} \times 0.6079 = 0.4053,
\\
\mathrm{PSL}_{B} = \frac{1/2}{(1/1 + 1/2)} \times 0.6079 = 0.2026
$$&lt;p&gt;
再映射回预算：
&lt;/p&gt;
$$
\epsilon_{A} = \frac{\gamma}{\mathrm{PSL}_{A}} = \frac{0.5}{0.4053} \approx 1.23
, \\
\epsilon_{B} = \frac{\gamma}{\mathrm{PSL}_{B}} = \frac{0.5}{0.2026} \approx 2.47
$$&lt;p&gt;
&lt;strong&gt;PSL 方法通过 P-series 为敏感位置分配递减预算，再结合距离与节点度为邻居分配个性化预算，并支持时间动态调整，实现“重要位置多留数据，敏感位置多加噪声”的精细化隐私保护。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        
    </channel>
</rss>
